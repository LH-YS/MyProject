# 实时部分的重点实现（作者LY）

## 一、Flink CDC监控配置表变化

​	项目中maxwell将数据全部写入一个topic，实时数仓要建立维度表就要从topic中筛选出需要的数据到对应的表中。而需要筛选哪些数据（来源表、目标表和字段），可以在外部配置表中配置，Flink任务实时读取。配置方案有三种：zookeeper通过Watch感知、mysql周期读取、监控mysql变化使用广播流。

​	FlinkCDC是基于Flink的可以获取全量和增量数据的source组件，我的项目中用了FlinkCDC的MySqlSource监控外部配置表，读取的每条数据为String，所以MapStateDescriptor指定String类作为key，自定义的配置表结构类作为value，建立广播流并连接主流，最后根据配置信息处理主流。

```java
		//将配置流作为广播流
        MapStateDescriptor<String, TableProcess> mapStateDescriptor = new MapStateDescriptor<>("map-state", String.class, TableProcess.class);
        BroadcastStream<String> broadcastStream = mysqlSourceDS
                .broadcast(mapStateDescriptor);
        //接主流与广播流
        BroadcastConnectedStream<JSONObject, String> connectedStream = filterJsonObjDS.connect(broadcastStream);
        //处理连接流，根据配置信息处理主流数据
        SingleOutputStreamOperator<JSONObject> dimDS = connectedStream.process(
                new TableProcessFunction(mapStateDescriptor)
        );
```

​	`TableProcessFunction`类继承`BroadcastProcessFunction`类，主要实现两个方法：`processElement`方法处理主流数据，`processBroadcastElement`方法处理广播流。主流数据需要获取广播流配置信息组装新json，写入hbase；而当配置表发生变化时，需要更新状态重新广播出去，这样新数据才能正确写入维度表（如果有新表需要在hbase中新建）。

```java
		public void processElement(JSONObject jsonObject, BroadcastProcessFunction<JSONObject, String, JSONObject>.ReadOnlyContext readOnlyContext, Collector<JSONObject> collector) throws Exception {
        //获取广播的配置信息
        ReadOnlyBroadcastState<String, TableProcess> broadcastState = readOnlyContext.getBroadcastState(mapStateDescriptor);
        //主流数据来自topic
        //{"database":"gmall-flink","table":"base_trademark","type":"insert","ts":1652499161,"xid":167,"commit":true,"data":{"id":13,"tm_name":"atguigu","logo_url":"/aaa/aaa"}}
        TableProcess tableProcess = broadcastState.get(jsonObject.getString("table"));//有可能为null
        //因为主流里可能有其他表，而配置信息里没有这些表，因为返回值可能为null，如果直接传参数会空指针异常
        //过滤字段 filterColumn
        //状态里面有才要，没有就不要，过滤出需要的维表信息
        if (tableProcess!=null) {
            filterColumn(jsonObject.getJSONObject("data"),tableProcess.getSink_columns());
            //补充SinkTable字段并写出到流中
            jsonObject.put("sinkTable",tableProcess.getSink_table());
            collector.collect(jsonObject);
        } else {
            System.out.println("找不到对应的key:"+jsonObject.getString("table"));
        }
    }


		public void processBroadcastElement(String s, BroadcastProcessFunction<JSONObject, String, JSONObject>.Context context, Collector<JSONObject> collector) throws Exception {
        //获取解析数据方便主流操作
        JSONObject jsonObject = JSON.parseObject(s);
        TableProcess tableProcess = JSON.parseObject(jsonObject.getString("after"), TableProcess.class);

        //校验表是否存在，如果不存在需要在phoenix中建表 checkTable（实践中sql语句中加上if not exist
        checkTable(tableProcess.getSink_table(),
                tableProcess.getSink_columns(),
                tableProcess.getSink_pk(),
                tableProcess.getSink_extend());
        //写入状态，广播出去
        BroadcastState<String, TableProcess> broadcastState = context.getBroadcastState(mapStateDescriptor);
        broadcastState.put(tableProcess.getSourceTable(),tableProcess);
    }
```

## 二、键控状态校验新老访客

​	现在市面上清理缓存的软件很多，对于前端埋点，可能会因为缓存清理导致新老访客标识初始化。项目里运用 Flink 状态编程，为每个 mid 维护一个键控状态，记录首次访问日期。如果日志流中is_new字段为1，键控状态为0则更新，为1则置is_new=0;如果日志流中is_new字段为0，键控状态为0则置首次访问ts为昨日（说明访问 APP 的是老访客，但本次是该访客的页面日志首次进入程序），为1则无视。	

```java
		//按照mid做分组
        KeyedStream<JSONObject, String> keyedStream = jsonObjDS.keyBy(json -> json.getJSONObject("common").getString("mid"));
		
		//使用状态编程做新老访客的校验
        SingleOutputStreamOperator<JSONObject> jsonObjWithNewTagDS = keyedStream.map(
                new RichMapFunction<JSONObject, JSONObject>() {
                    private ValueState<String> lastVisitState;

                    @Override
                    public void open(Configuration parameters) throws Exception {
                        lastVisitState = getRuntimeContext().getState(
                                new ValueStateDescriptor<>("last-visit", String.class)
                        );
                    }

                    @Override
                    public JSONObject map(JSONObject value) throws Exception {
                        //获取is_new标记
                        String isNew = value.getJSONObject("common").getString("is_new");
                        Long ts = value.getLong("ts");
                        //当前这条数据的日期
                        String curDate = DateFormatter.toDate(ts);
                        //获取状态中的日期
                        String lastDate = lastVisitState.value();
                        if ("1".equals(isNew)) {
                            if (lastDate == null) {
                                lastVisitState.update(curDate);//状态记录数据不变
                            } else if (!lastDate.equals(curDate)) {
                                //状态不为null且不是当天，说明是卸载重装的用户
                                value.getJSONObject("common").put("is_new", "0");
                            }
                        } else {
                            if (lastDate == null) {
                                lastVisitState.update(DateFormatter.toDate(ts - 24 * 60 * 60 * 1000L));
                            }
                        }

                        return value;
                    }
                }
        );
```

​	先按照mid对数据分组，再对keyedStream进行map。open方法初始化状态，状态用String保存，逻辑判断后维护状态。

## 三、独立访客需求（状态存活）

​	与上面同理访客用键控状态维护，但不同的是独立访客需要区别每日。对于独立访客，last_page_id必定为null，过滤后跟上面的思路一样，但需要设置TTL为1天，模式为OnCreateAndWrite（创建和更新状态时重置状态存活时间）。	

```java
				//设置状态TTL，在open方法中
                StateTtlConfig ttlConfig = new StateTtlConfig.Builder(Time.days(1))
                        .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite).build();
                stateDescriptor.enableTimeToLive(ttlConfig);

```

## 四、Lookup Cache和Lookup Join

​	Lookup Cache开启后每个TaskManager进程会维护一份缓存，Flink未命中缓存才会查表，并更新缓存。Lookup Join 通常在 Flink SQL 表和外部系统查询结果关联时使用（项目中加购等事务用到，关联外部字典表时也经常用）。

```sql
String ddl = "WITH ( " +
                "'connector' = 'jdbc', " +
                "'url' = 'jdbc:mysql://hadoop102:3306/gmall', " +
                "'table-name' = '" + tableName + "', " +
                "'lookup.cache.max-rows' = '10', " +
                "'lookup.cache.ttl' = '1 hour', " +
                "'username' = 'root', " +
                "'password' = 'adysjy521', " +
                "'driver' = 'com.mysql.cj.jdbc.Driver' " +
                ")";
```

## 五、项目中多流多指标粒度的统计场景

​	多流合并常见的有union()，connect()，intervalJoin。

​	DWS层中一些任务是需要统计多个指标粒度，从多个流中读取数据。显然多流join是不划算的，项目中采用了统一处理的思路，先创建一个最终结果的实体类，然后分别将三流转化成实体类（此时其余字段为0，计数字段为1），然后union三个流，最后开窗聚合。

​	除了要关注窗口延时关闭外，还要注意如果中间断了数据，统计将重新开始，因为union聚合依赖的是每条数据加上新一条数据，而不是状态保存。

## 六、与维度表关联时的旁路缓存优化和异步IO

​	项目与维度表关联时，维度表保存在HBase，但在流处理场景下查询的反复序列化导致效率依旧不够，因此引入旁路缓存来提高查询效率。

​	项目中简单运用了redis，运用旁路缓存要注意两点：防止冷数据阻塞和及时变更。对于冷数据，设置超时时间为12h；对于及时变更，思路是当维度表中数据操作类型为update，则清除对应的缓存数据。

​	具体实现是在维度表写入时的连接方法里检测缓存的更新方式：

```java
@Override
    public void invoke(JSONObject jsonObj, Context context) throws Exception {
        // 获取目标表表名
        String sinkTable = jsonObj.getString("sinkTable");
		......
        // 如果操作类型为 update，则清除 redis 中的缓存信息
        if ("update".equals(type)) {
            DimUtil.deleteCached(sinkTable, id);
        }
    }
```

​	异步IO主要继承RichAsyncFunction函数，实现asyncInvoke方法，项目中用德鲁伊连接池。

```java
 @Override
    public void asyncInvoke(T obj, ResultFuture<T> resultFuture) throws Exception {
        //从线程池中获取线程，发送异步请求
        executorService.submit(
                new Runnable() {
                    @Override
                    public void run() {
                        try {
                            // 1. 根据流中的对象获取维度的主键
                            String key = getKey(obj);

                            // 2. 根据维度的主键获取维度对象
                            // 获取数据库连接对象
                            DruidPooledConnection conn = druidDataSource.getConnection();

                            JSONObject dimJsonObj = null;
                            try {
                                dimJsonObj = DimUtil.getDimInfo(conn, tableName, key);
                            } catch (Exception e) {
                                System.out.println("维度数据异步查询异常");
                                e.printStackTrace();
                            } finally {
                                try {
                                    // 归还数据库连接对象
                                    conn.close();
                                } catch (SQLException sqlException) {
                                    System.out.println("数据库连接对象归还异常");
                                    sqlException.printStackTrace();
                                }
                            }

                            // 3. 将查询出来的维度信息 补充到流中的对象属性上
                            if (dimJsonObj != null) {
                                join(obj, dimJsonObj);
                            }
                            resultFuture.complete(Collections.singleton(obj));
                        } catch (Exception e) {
                            e.printStackTrace();
                            throw new RuntimeException("异步维度关联发生异常了");
                        }
                    }
                }
        );

```

## 七、项目中TTL设置

​	对于每个业务流程，TTL的设置也有所不同，这里简单写一下重点几个。

​	1、字典表join时，主表TTL设置为5s。此时如果未命中需要更新lookup缓存，TTL可以设置短点。

​	2、订单明细表、订单表、订单明细优惠券管理表和订单明细活动关联表不存在业务上的滞后问题，只考虑可能的数据乱序即可，将 ttl 设置为 5s。

​	3、通常支付操作在下单后15min内完成即可，因此，支付明细数据可能比订单明细数据滞后 15min。考虑到可能的乱序问题，ttl 设置为 15min + 5s。