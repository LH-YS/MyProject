# 采集时的重点实现（作者LY）

## 一、Flume的数据漂移问题解决

​	数据到达从kafka到达Flume需要消耗一定时间，如果前一天的23:59发出，很有可能第二天才能到达导致落库错误，需要把kafka source的header中的时间修改成文件自有的时间，这样最终就可以正确落盘hive了。

​	具体是写一个Flume拦截器，实现Flume包下的Interceptor接口，重点实现intercept方法：

```java
	@Override
    public Event intercept(Event event) {

		//1、获取header和body的数据
        Map<String, String> headers = event.getHeaders();
        String log = new String(event.getBody(), StandardCharsets.UTF_8);

		//2、将body的数据类型转成jsonObject类型（方便获取数据）
        JSONObject jsonObject = JSONObject.parseObject(log);

		//3、header中timestamp时间字段替换成日志生成的时间戳（解决数据漂移问题）
        String ts = jsonObject.getString("ts");
        headers.put("timestamp", ts);

        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        for (Event event : list) {
            intercept(event);
        }
        return list;
    }
```

​	对list中每一个event都进行提取处理，格式化body为json格式，用body中的时间替换header中的时间。打包好放在lib里，使用时在conf里配置。

​	另外，对于增量表的Flume拦截器，Maxwell输出的数据中时间戳的单位为秒，而HDFSSink的要求为毫秒，需要乘1000。

## 二、Maxwell

​	Maxwell读取的是binlog文件，穿到下游时的时间戳也是binlog的时间戳，而binlog的时间并不代表目标表的时间，比如要导入的是1月1日的表，而binlog的时间是1月3日，最终到hdfs时落盘在1月3日的文件夹中，这显然是不符合需求的，因此我这里用了网上可以通过配置指定传到下游的时间戳的maxwell修改版本（也可以自己改源码，CSDN上有教程，后续有时间了会更新此笔记）。